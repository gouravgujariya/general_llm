{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The main process for the creation of the general llm contains steps\n",
        "1. The structure of data that we are going to use for the training of the llm\n",
        "2. base llm model which we want to train for our specific use-case\n",
        "\n",
        "# The main processing work can be followed by -\n",
        "1. data loading\n",
        "2. data preprocessing(auto tokenizer)\n",
        "3. peft(paremeter efficient fine tuning) and lora\n",
        "4. model's arguments adjustments\n",
        "5. model training\n",
        "6. model saving\n",
        "7. if want(model + peft merging for new model)\n",
        "8. saving the model to get used"
      ],
      "metadata": {
        "id": "PI8QcNoyWXjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terminologies you should know before\n",
        "## Parameter efficent transfer learning\n",
        "## Parameter efficent fine tuning lora\n",
        "## need of load and qlora"
      ],
      "metadata": {
        "id": "gIr4HpZTYAzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "2Xn-aR_mUimK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading function"
      ],
      "metadata": {
        "id": "W_fLq_v9YY_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzfhgm7OV-2X"
      },
      "outputs": [],
      "source": [
        "def load_raw_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    return data\n",
        "\n",
        "def load_question_answer(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        qa_data = file.read()\n",
        "        qa_texts = [f\"Question: {qa['question']} Answer: {qa['answer']}\" for qa in qa_data]\n",
        "    return data\n",
        "\n",
        "def load_chunk(file_path,chunk_size):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        chunk = file.read(chunk_size)\n",
        "    return chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PEFT and LORA arguments setting"
      ],
      "metadata": {
        "id": "Js2IR4LyYfgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fine_tune_llm:\n",
        "  def __init__(Model_to_be_used_name=\"NousResearch/Llama-2-7b-chat-hf\",new_model_name=\"Llama-2-7b-chat-finetune\"):\n",
        "    # Model and dataset\n",
        "    model_name = Model_to_be_used_name\n",
        "    new_model = new_model_name\n",
        "\n",
        "    # QLoRA parameters\n",
        "    lora_r = 64\n",
        "    lora_alpha = 16\n",
        "    lora_dropout = 0.1\n",
        "\n",
        "    # bitsandbytes parameters\n",
        "    use_4bit = True\n",
        "    bnb_4bit_compute_dtype = \"float16\"\n",
        "    bnb_4bit_quant_type = \"nf4\"\n",
        "    use_nested_quant = False\n",
        "\n",
        "    # TrainingArguments parameters\n",
        "    output_dir = \"./results\"\n",
        "    num_train_epochs = 1\n",
        "    fp16 = False\n",
        "    bf16 = False\n",
        "    per_device_train_batch_size = 4\n",
        "    per_device_eval_batch_size = 4\n",
        "    gradient_accumulation_steps = 1\n",
        "    gradient_checkpointing = True\n",
        "    max_grad_norm = 0.3\n",
        "    learning_rate = 2e-4\n",
        "    weight_decay = 0.001\n",
        "    optim = \"paged_adamw_32bit\"\n",
        "    lr_scheduler_type = \"cosine\"\n",
        "    max_steps = -1\n",
        "    warmup_ratio = 0.03\n",
        "    group_by_length = True\n",
        "    save_steps = 0\n",
        "    logging_steps = 25\n",
        "    max_seq_length = None\n",
        "    packing = False\n",
        "    device_map = {\"\": 0}\n",
        "\n",
        "  def load_config(self):\n",
        "    compute_dtype = getattr(torch, self.bnb_4bit_compute_dtype)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=self.use_4bit,\n",
        "        bnb_4bit_quant_type=self.bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype=self.compute)\n",
        "    return compute_dtype,bnb_config\n",
        "\n",
        "  def check_gpu_compatibility(self,compute_dtype):\n",
        "    if compute_dtype == torch.float16 and use_4bit:\n",
        "      major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "  def load_base_model(self,bnb_config,):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        self.model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=self.device_map)\n",
        "    self.model.config.use_cache = False\n",
        "    self.model.config.pretraining_tp = 1\n",
        "    return base_model\n",
        "\n",
        "  def LLama_tokenizer(self):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(self.model_name,trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    return tokenizer\n",
        "\n",
        "  def Lora_config(self):\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=self.lora_alpha,\n",
        "        lora_dropout=self.lora,\n",
        "        r=self.lora_r,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\")\n",
        "    return peft_config\n",
        "\n",
        "  def set_training_parameter(self):\n",
        "    training_arguments = TrainingArguments(\n",
        "      output_dir=self.output_dir,\n",
        "      num_train_epochs=self.num_train_epochs,\n",
        "      per_device_train_batch_size=self.per_device_train_batch_size,\n",
        "      gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
        "      optim=self.optim,\n",
        "      save_steps=self.save_steps,\n",
        "      logging_steps=self.logging_steps,\n",
        "      learning_rate=self.learning_rate,\n",
        "      weight_decay=self.weight_decay,\n",
        "      fp16=self.fp16,\n",
        "      bf16=self.bf16,\n",
        "      max_grad_norm=self.max_grad_norm,\n",
        "      max_steps=self.max_steps,\n",
        "      warmup_ratio=self.warmup_ratio,\n",
        "      group_by_length=self.group_by_length,\n",
        "      lr_scheduler_type=self.lr_scheduler_type,\n",
        "      report_to=\"tensorboard\"\n",
        "      )\n",
        "    return training_arguments\n",
        "\n",
        "  def set_fine_tuning_parameters(self,model,dataset,tokenizer,peft_config,training_arguments):\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=None,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        "        packing=False\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "  def model_save(self,Trainer):\n",
        "    return Trainer.model.save_pretrained(self.new_model);\n",
        "\n"
      ],
      "metadata": {
        "id": "_0T8oJP6NFBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "e4-wRXEvYkU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning with questions and answer data"
      ],
      "metadata": {
        "id": "4tYEO8xuXsL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a custom class or module named 'fine_tune_llm' with methods as described\n",
        "\n",
        "# Create an instance of the fine_tune_llm class/module\n",
        "a = fine_tune_llm()\n",
        "\n",
        "# Load the question-answer dataset using the corrected function\n",
        "data_path = \"your_data_path.json\"  # Ensure this points to your JSON file\n",
        "data = load_question_answer(data_path)\n",
        "\n",
        "# Load the configuration for compute_dtype and other related configurations\n",
        "compute_dtype, bnb_config = a.load_config()\n",
        "\n",
        "# Check GPU compatibility with the compute_dtype\n",
        "a.check_gpu_compatibility(compute_dtype)\n",
        "\n",
        "# Load the base model to be fine-tuned\n",
        "base_model = a.load_base_model()\n",
        "\n",
        "# Initialize the tokenizer specific to LLaMA\n",
        "tokenizer = a.LLama_tokenizer()\n",
        "\n",
        "# Set up PEFT configuration (e.g., for LoRA)\n",
        "peft_config = a.Lora_config()\n",
        "\n",
        "# Set training parameters for fine-tuning\n",
        "training_arguments = a.set_training_parameter()\n",
        "\n",
        "# Set fine-tuning parameters with the loaded data\n",
        "trainer = a.set_fine_tuning_parameters(base_model, data, tokenizer, peft_config, training_arguments)\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.model.save_pretrained(a.new_model)\n",
        "\n",
        "# Reload the model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    a.model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=a.device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, a.new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload the tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(a.model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "DH_29OZcXeU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine tuning with raw data"
      ],
      "metadata": {
        "id": "LnBDQPIdXzJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the fine-tuning class instance\n",
        "a = fine_tune_llm()\n",
        "\n",
        "# Path to your dataset (make sure itâ€™s properly formatted for fine-tuning)\n",
        "data_path = \"your_data_path\"\n",
        "\n",
        "# Load the raw data from the specified path\n",
        "data = load_raw_data(data_path)\n",
        "\n",
        "# Load the configuration settings, including compute dtype and bitsandbytes config\n",
        "compute_dtype, bnb_config = a.load_config()\n",
        "\n",
        "# Check if the GPU is compatible with the selected compute dtype\n",
        "a.check_gpu_compatibility(compute_dtype)\n",
        "\n",
        "# Load the base model that you want to fine-tune\n",
        "base_model = a.load_base_model()\n",
        "\n",
        "# Load the tokenizer associated with the LLaMA model\n",
        "tokenizer = a.LLama_tokenizer()\n",
        "\n",
        "# Load the LoRA (Low-Rank Adaptation) configuration for PEFT (Parameter-Efficient Fine-Tuning)\n",
        "peft_config = a.Lora_config()\n",
        "\n",
        "# Set the training parameters (e.g., learning rate, batch size, number of epochs)\n",
        "training_arguments = a.set_training_parameter()\n",
        "\n",
        "# Set up the fine-tuning trainer with the model, data, tokenizer, and training arguments\n",
        "trainer = a.set_fine_tuning_parameters(base_model, data, tokenizer, peft_config, training_arguments)\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model to the specified directory\n",
        "trainer.model.save_pretrained(a.new_model)\n",
        "\n",
        "# Reload the base model in FP16 (16-bit floating point) and merge it with the LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    a.model_name,  # The original base model's name\n",
        "    low_cpu_mem_usage=True,  # Optimize memory usage on CPU\n",
        "    return_dict=True,  # Ensure the model returns a dictionary\n",
        "    torch_dtype=torch.float16,  # Use FP16 precision\n",
        "    device_map=a.device_map,  # Map model to the appropriate device(s)\n",
        ")\n",
        "\n",
        "# Load the fine-tuned model with LoRA weights\n",
        "model = PeftModel.from_pretrained(base_model, a.new_model)\n",
        "\n",
        "# Merge LoRA weights with the base model and unload unnecessary parts\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload the tokenizer for the model to save it properly\n",
        "tokenizer = AutoTokenizer.from_pretrained(a.model_name, trust_remote_code=True)\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Ensure padding is applied to the right side of the sequence\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "rDE527wTYng6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "3gb0bM4LYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "EiEuxnRYYrsl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}